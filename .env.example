# =============================================================================
# SmartRent Scraper Configuration
# =============================================================================

# =============================================================================
# Application Environment
# =============================================================================
NODE_ENV=development
PORT=3000LOG_LEVEL=info

# =============================================================================
# Database Configuration
# =============================================================================
# MySQL connection string for Prisma
DATABASE_URL="mysql://username:password@localhost:3306/smartrent_scraper"

# =============================================================================
# Scraping Configuration
# URLs to scrape (comma-separated)
# Example: SCRAPING_URLS=https://site1.com/listings,https://site2.com/properties,https://site3.com/property/123
SCRAPING_URLS=https://example-rental-site.com/listings,https://example-rental-site.com/property/123

# =============================================================================
# Maximum number of concurrent scrapers to run
MAX_CONCURRENT_SCRAPERS=5

# Delay between requests in milliseconds
REQUEST_DELAY_MS=1000

# Number of retry attempts for failed requests
RETRY_ATTEMPTS=3

# Request timeout in milliseconds
TIMEOUT_MS=30000

# Run browser in headless mode (true/false)
HEADLESS=true

# Browser launch arguments (comma-separated)
BROWSER_ARGS=--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage

# =============================================================================
# Rate Limiting Configuration
# =============================================================================
# Maximum requests per minute
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# Burst limit for rate limiting
BURST_LIMIT=10

# =============================================================================
# Export Configuration
# =============================================================================
# Export format: json, csv, database, or combinations (comma-separated)
# Options: "json", "csv", "database", "json,csv", "all"
EXPORT_FORMAT=json,csv,database

# Directory for data output files
DATA_OUTPUT_DIR=./data

# Directory for log output files
LOG_OUTPUT_DIR=./logs

# =============================================================================
# Proxy Configuration (Optional)
# =============================================================================
# PROXY_URL=http://proxy.example.com:8080
# PROXY_USERNAME=your_proxy_username
# PROXY_PASSWORD=your_proxy_password

# =============================================================================
# External Services (Optional)
# =============================================================================
# Redis URL for caching (if implemented)
# REDIS_URL=redis://localhost:6379

# Webhook URL for notifications (if implemented)
# WEBHOOK_URL=https://your-webhook-endpoint.com/scraping-results

# =============================================================================
# Development/Testing Configuration
# =============================================================================
# Enable debug logging (true/false)
DEBUG=false

# Test mode - limits scraping for development
TEST_MODE=false

# Maximum pages to scrape in test mode
MAX_TEST_PAGES=5

# =============================================================================
# Performance Configuration
# =============================================================================
# Memory limit for scraping operations (MB)
MEMORY_LIMIT=512

# Maximum file size for exports (MB)
MAX_EXPORT_FILE_SIZE=100

# Cleanup old data after N days
CLEANUP_DAYS=30
